import re
from ast import Await

import requests
from aiogram import types
from bs4 import BeautifulSoup

from main import bot


def get_url(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    return soup


class Information:
    # –ü–∞—Ä—Å–∏–Ω–≥ 10 –ø–µ—Ä—à–∏—Ö —Ñ–æ—Ç–æ
    def get_photo(soup: BeautifulSoup, a_lot_of: bool) -> list | types.URLInputFile:
        photo = soup.find("div", class_="swiper-wrapper").find_all("img")

        list_src_photo = []
        media_group = []

        for src in photo:
            list_src_photo.append(src.get("src"))

        if len(list_src_photo) > 10:
            del list_src_photo[10:]

        for photo_url in list_src_photo:
            media_group.append(types.InputMediaPhoto(media=photo_url))

        first_photo = types.URLInputFile(str(list_src_photo[0]))

        if not a_lot_of:
            return first_photo

        return media_group

    # –ü–∞—Ä—Å–∏–Ω–≥ –≥–æ–ª–æ–≤–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó (–∫-—Ç—å –∫—ñ–º–Ω–∞—Ç, –ø–æ–≤–µ—Ä—Ö, –ø–ª–æ—â–∞, –†–∞–π–æ–Ω)
    def get_main_information(soup: BeautifulSoup) -> [str, str, str, str]:
        # constants to check the list "tags"
        need_words_ukrainian = [
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—ñ–º–Ω–∞—Ç:",
            "–ó–∞–≥–∞–ª—å–Ω–∞ –ø–ª–æ—â–∞:",
            "–ü–æ–≤–µ—Ä—Ö:",
            "–ü–æ–≤–µ—Ä—Ö–æ–≤—ñ—Å—Ç—å:",
        ]
        need_words_russian = [
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç:",
            "–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å:",
            "–≠—Ç–∞–∂:",
            "–≠—Ç–∞–∂–Ω–æ—Å—Ç—å:",
        ]

        checklist = []
        tags = soup.find("ul", class_="css-sfcl1s").find_all("p")

        for need_word in need_words_russian:
            for tag in tags:
                if need_word in tag.text:
                    checklist.append(tag.text)

        for need_word in need_words_ukrainian:
            for tag in tags:
                if need_word in tag.text:
                    checklist.append(tag.text)

        try:
            if len(checklist) != 4:
                rooms = re.search(r"\d+", checklist[0]).group()
                area = re.search(r"\d+", checklist[1]).group()
                find_everything = re.search(r"\d+", checklist[2])
                flour = f"{find_everything.group()}"
            else:
                rooms = re.search(r"\d+", checklist[0]).group()
                area = re.search(r"\d+", checklist[1]).group()
                find_have = re.search(r"\d+", checklist[2])
                find_everything = re.search(r"\d+", checklist[3])
                flour = f"{find_have.group()} –∑ {find_everything.group()}"
        except:
            rooms, area, flour = "", "", ""

        # -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

        # we are looking for a district and a city, because
        # if there is no district, then the city is a district

        find = soup.find_all("script")
        pattern_district = re.compile(r'\\"districtName\\":\\"([^\\"]+)\\"')
        pattern_city = re.compile(r'\\"cityName\\":\\"([^\\"]+)\\"')

        for one in find:
            district = pattern_district.search(one.text)
            if district:
                break

        for one in find:
            city = pattern_city.search(one.text)
            if city:
                break

        if district:
            district = district.group(1)
        elif city:
            district = city.group(1)
        else:
            district = ""

        return rooms, flour, area, district

    def get_price(soup: BeautifulSoup) -> [str, str]:
        # parsing price from the page
        price = soup.find("h2", text=re.compile(r".*–≥—Ä–Ω.*"))

        if not price:
            price = soup.find("h2", text=re.compile(r".*\$.*"))

        if not price:
            price = soup.find("h3", text=re.compile(r".*–≥—Ä–Ω.*"))

        if not price:
            price = soup.find("h3", text=re.compile(r".*\$.*"))

        if not price:
            price = soup.find("h4", text=re.compile(r".*–≥—Ä–Ω.*"))

        if not price:
            price = soup.find("h4", text=re.compile(r".*\$.*"))

        if not price:
            return "–°—É–º—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"

        return price.text

    def delete_words(text: str, words_to_remove: list) -> str:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–µ–≥—É–ª—è—Ä–Ω–∏–π –≤–∏—Ä–∞–∑ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å–ª–æ–≤–∞ –∑ –º–æ–∂–ª–∏–≤–∏–º–∏ –∫—Ä–∞–ø–∫–∞–º–∏
        pattern = re.compile(
            r"\b(?:" + "|".join(map(re.escape, words_to_remove)) + r")\b", re.IGNORECASE
        )

        # –ó–∞–º—ñ–Ω—é—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ —Å–ª–æ–≤–∞ –Ω–∞ –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏
        result = pattern.sub("", text)

        return result

    def get_header(soup: BeautifulSoup) -> [str, str]:
        # parsing caption from the page
        header = soup.find("h4", class_="css-1juynto")

        if not header:
            return None

        return header.text

    def get_caption(soup: BeautifulSoup) -> str:
        # parsing caption from the page
        caption = soup.find("div", class_="css-1t507yq er34gjf0")

        if not caption:
            return "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        if len(caption.text) > 800:
            return caption.text[0:800]

        return caption.text

    def create_caption(soup: BeautifulSoup) -> str:
        words = [
            "–í—ñ–¥",
            "–û—Ç",
            "—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫",
            "—è –≤–ª–∞—Å–Ω–Ω–∏–∫",
            "–ø–æ—Å—Ä–µ–¥–Ω–∏–∫–æ–≤",
            "—Å–≤–æ—è",
            "—Å–≤–æ—é",
            "—Ä–∏–µ–ª—Ç–æ—Ä",
            "—Ä–∏–µ–ª—Ç–æ—Ä–æ–≤",
            "–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ",
            "–∞–≥–µ–Ω—Ç",
            "–º–∞–∫–ª–µ—Ä",
            "–ø–æ—Å—Ä–µ–¥–Ω–∏–∫",
            "–ª–∏—á–Ω—É—é",
            "—Ö–æ–∑—è–∏–Ω",
            "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫",
            "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞",
            "—Ö–æ–∑—è–∏–Ω–∞",
            "—Ö–æ–∑—è–π–∫–∞",
            "–±–µ–∑ –∫–æ–º–∏—Å—Å–∏–∏",
            "–∞–≥–µ–Ω—Ç–∞",
            "–∞–≥–µ–Ω—Ç—Å—Ç–≤–∞",
            "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤",
            "–ø–æ—Å–µ—Ä–µ–¥–Ω–∏–∫—ñ–≤",
            "—Å–≤–æ—è",
            "—Å–≤–æ—é",
            "—Ä—ñ–µ–ª—Ç–æ—Ä",
            "—Ä—ñ–µ–ª—Ç–æ—Ä—ñ–≤",
            "–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ",
            "–∞–≥–µ–Ω—Ç",
            "–º–∞–∫–ª–µ—Ä",
            "–ø–æ—Å–µ—Ä–µ–¥–Ω–∏–∫",
            "–ø–æ—Å–µ—Ä–µ–¥–Ω–∏–∫",
            "–æ—Å–æ–±–∏—Å—Ç—É",
            "–≤–ª–∞—Å–Ω–∏–∫",
            "–≤–ª–∞—Å–Ω–∏–∫–∞",
            "–≤–ª–∞—Å–Ω–∏–∫—ñ–≤",
            "—Ö–∞–∑—è—ó–Ω–∞—Ö–∞–∑—è–π–∫–∞",
            "—Ö–∞–∑—è–π–∫–∞",
            "–æ—Å–æ–±–∏—Å—Ç—É",
            "–±–µ–∑ –∫–æ–º—ñ—Å—ñ—ó",
            "–ë–µ–∑ —Ä—ñ—î–ª—Ç–æ—Ä—ñ–≤",
            "–∫–æ–º—ñ—Å—ñ–π",
            "–ë–µ–∑ —Ä–∏–µ–ª—Ç–æ—Ä–æ–≤",
            "–∫–æ–º–∏—Å–∏–π",
            "–∫–æ–º—ñ—Å—ñ–á",
            "–∫–æ–º–∏—Å–∏–∏",
        ]

        caption = Information.delete_words(Information.get_caption(soup), words)
        header = Information.delete_words(Information.get_header(soup), words)

        rooms, flour, area, district = Information.get_main_information(soup)
        money = Information.get_price(soup)

        captions = (
            f"üè°{rooms}–∫ –∫–≤\n" f"üè¢–ü–æ–≤–µ—Ä—Ö: {flour}\n" f"üîë–ü–ª–æ—â–∞: {area}–º2\n" f"üìç–†–∞–π–æ–Ω: {district}\n"
        )

        main_caption = f"üí≥Ô∏è{money}" f"\n\n{header}\n\n" f"üìù–û–ø–∏—Å:\n{caption}"
        if not rooms != "":
            return main_caption
        return captions + main_caption


# –û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö —ñ –∑–∞–ø—É—Å–∫ –Ω–∞–¥—Å–∏–ª–∞–Ω–Ω—è
async def get_data(message: types.Message):
    soup: BeautifulSoup = get_url(message.text)
    photo_group = Information.get_photo(soup, True)
    caption = Information.create_caption(soup)

    message_photo = await message.answer_media_group(media=photo_group)
    await bot.edit_message_caption(
        message_id=message_photo[0].message_id,
        chat_id=message.chat.id,
        caption=caption,
        parse_mode="HTML",
    )
